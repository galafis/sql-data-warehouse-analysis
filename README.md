# ğŸ‡§ğŸ‡· AnÃ¡lise de Data Warehouse com SQL AvanÃ§ado | ğŸ‡ºğŸ‡¸ Advanced SQL Data Warehouse Analysis

<div align="center">

![SQL](https://img.shields.io/badge/SQL-4479A1?style=for-the-badge&logo=postgresql&logoColor=white)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-316192?style=for-the-badge&logo=postgresql&logoColor=white)
![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![dbt](https://img.shields.io/badge/dbt-FF694B?style=for-the-badge&logo=dbt&logoColor=white)
![Apache Airflow](https://img.shields.io/badge/Apache%20Airflow-017CEE?style=for-the-badge&logo=apache-airflow&logoColor=white)

**Plataforma completa de anÃ¡lise de data warehouse com SQL avanÃ§ado e ETL moderno**

[ğŸ“Š AnÃ¡lises](#-anÃ¡lises-disponÃ­veis) â€¢ [ğŸ—ï¸ Arquitetura](#-arquitetura) â€¢ [âš¡ Setup](#-setup-rÃ¡pido) â€¢ [ğŸ“ˆ Dashboards](#-dashboards)

</div>

---

## ğŸ‡§ğŸ‡· PortuguÃªs

### ğŸ“Š VisÃ£o Geral

Plataforma abrangente de **anÃ¡lise de data warehouse** desenvolvida com SQL avanÃ§ado e ferramentas modernas de ETL:

- ğŸ—ï¸ **Arquitetura Dimensional**: Star schema, snowflake, data vault
- ğŸ“Š **SQL AvanÃ§ado**: CTEs, window functions, anÃ¡lises complexas
- ğŸ”„ **ETL Moderno**: dbt para transformaÃ§Ãµes, Airflow para orquestraÃ§Ã£o
- ğŸ“ˆ **Analytics**: KPIs, mÃ©tricas de negÃ³cio, anÃ¡lises temporais
- ğŸ¯ **Performance**: OtimizaÃ§Ã£o de queries, indexaÃ§Ã£o, particionamento
- ğŸ“‹ **GovernanÃ§a**: Qualidade de dados, linhagem, documentaÃ§Ã£o

### ğŸ¯ Objetivos da Plataforma

- **Centralizar dados** de mÃºltiplas fontes operacionais
- **Padronizar mÃ©tricas** e definiÃ§Ãµes de negÃ³cio
- **Acelerar anÃ¡lises** com estruturas otimizadas
- **Garantir qualidade** e consistÃªncia dos dados
- **Facilitar self-service** analytics para usuÃ¡rios

### ğŸ› ï¸ Stack TecnolÃ³gico

#### Database e Storage
- **PostgreSQL**: Database principal do data warehouse
- **Amazon Redshift**: Data warehouse na nuvem (opcional)
- **Snowflake**: Plataforma de dados moderna (opcional)
- **Apache Parquet**: Formato colunar para storage

#### ETL e TransformaÃ§Ã£o
- **dbt (data build tool)**: TransformaÃ§Ãµes SQL como cÃ³digo
- **Apache Airflow**: OrquestraÃ§Ã£o de workflows
- **Pandas**: Processamento de dados em Python
- **SQLAlchemy**: ORM e conexÃµes de banco

#### Qualidade e GovernanÃ§a
- **Great Expectations**: ValidaÃ§Ã£o de qualidade de dados
- **Apache Atlas**: CatÃ¡logo de dados e linhagem
- **dbt docs**: DocumentaÃ§Ã£o automÃ¡tica
- **Soda**: Monitoramento de qualidade

#### VisualizaÃ§Ã£o e BI
- **Apache Superset**: Dashboards e visualizaÃ§Ãµes
- **Grafana**: Monitoramento e mÃ©tricas
- **Jupyter**: AnÃ¡lises exploratÃ³rias
- **Plotly**: GrÃ¡ficos interativos

### ğŸ“‹ Arquitetura do Data Warehouse

```
sql-data-warehouse-analysis/
â”œâ”€â”€ ğŸ“ sql/                        # Scripts SQL organizados
â”‚   â”œâ”€â”€ ğŸ“ ddl/                    # Data Definition Language
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ create_schemas.sql  # CriaÃ§Ã£o de schemas
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ create_tables.sql   # CriaÃ§Ã£o de tabelas
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ create_indexes.sql  # CriaÃ§Ã£o de Ã­ndices
â”‚   â”‚   â””â”€â”€ ğŸ“„ create_views.sql    # CriaÃ§Ã£o de views
â”‚   â”œâ”€â”€ ğŸ“ dml/                    # Data Manipulation Language
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ insert_dimensions.sql # Carga de dimensÃµes
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ insert_facts.sql    # Carga de fatos
â”‚   â”‚   â””â”€â”€ ğŸ“„ update_scd.sql      # Slowly Changing Dimensions
â”‚   â”œâ”€â”€ ğŸ“ analytics/              # Queries analÃ­ticas
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ sales_analysis.sql  # AnÃ¡lise de vendas
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ customer_analysis.sql # AnÃ¡lise de clientes
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ product_analysis.sql # AnÃ¡lise de produtos
â”‚   â”‚   â””â”€â”€ ğŸ“„ financial_analysis.sql # AnÃ¡lise financeira
â”‚   â”œâ”€â”€ ğŸ“ procedures/             # Stored procedures
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ sp_load_daily.sql   # Carga diÃ¡ria
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ sp_data_quality.sql # VerificaÃ§Ã£o qualidade
â”‚   â”‚   â””â”€â”€ ğŸ“„ sp_aggregations.sql # AgregaÃ§Ãµes
â”‚   â””â”€â”€ ğŸ“ functions/              # User-defined functions
â”‚       â”œâ”€â”€ ğŸ“„ date_functions.sql  # FunÃ§Ãµes de data
â”‚       â”œâ”€â”€ ğŸ“„ string_functions.sql # FunÃ§Ãµes de string
â”‚       â””â”€â”€ ğŸ“„ business_functions.sql # FunÃ§Ãµes de negÃ³cio
â”œâ”€â”€ ğŸ“ dbt/                        # Projeto dbt
â”‚   â”œâ”€â”€ ğŸ“ models/                 # Modelos dbt
â”‚   â”‚   â”œâ”€â”€ ğŸ“ staging/            # Camada staging
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ stg_customers.sql # Staging clientes
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ stg_orders.sql  # Staging pedidos
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ stg_products.sql # Staging produtos
â”‚   â”‚   â”œâ”€â”€ ğŸ“ intermediate/       # Camada intermediÃ¡ria
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ int_customer_orders.sql # Pedidos por cliente
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ int_product_sales.sql # Vendas por produto
â”‚   â”‚   â”œâ”€â”€ ğŸ“ marts/              # Data marts
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ dim_customer.sql # DimensÃ£o cliente
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ dim_product.sql # DimensÃ£o produto
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ“„ dim_date.sql    # DimensÃ£o data
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ fact_sales.sql  # Fato vendas
â”‚   â”‚   â””â”€â”€ ğŸ“ analytics/          # Modelos analÃ­ticos
â”‚   â”‚       â”œâ”€â”€ ğŸ“„ customer_metrics.sql # MÃ©tricas cliente
â”‚   â”‚       â”œâ”€â”€ ğŸ“„ product_performance.sql # Performance produto
â”‚   â”‚       â””â”€â”€ ğŸ“„ sales_summary.sql # Resumo vendas
â”‚   â”œâ”€â”€ ğŸ“ macros/                 # Macros dbt
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ generate_schema_name.sql # Schema naming
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ test_not_null_where.sql # Testes customizados
â”‚   â”‚   â””â”€â”€ ğŸ“„ pivot_table.sql     # Macro pivot
â”‚   â”œâ”€â”€ ğŸ“ tests/                  # Testes de dados
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ assert_positive_values.sql # Valores positivos
â”‚   â”‚   â””â”€â”€ ğŸ“„ assert_valid_dates.sql # Datas vÃ¡lidas
â”‚   â”œâ”€â”€ ğŸ“„ dbt_project.yml         # ConfiguraÃ§Ã£o dbt
â”‚   â””â”€â”€ ğŸ“„ profiles.yml            # Perfis de conexÃ£o
â”œâ”€â”€ ğŸ“ airflow/                    # DAGs Airflow
â”‚   â”œâ”€â”€ ğŸ“ dags/                   # DefiniÃ§Ã£o de DAGs
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ daily_etl_dag.py    # ETL diÃ¡rio
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ weekly_aggregation_dag.py # AgregaÃ§Ãµes semanais
â”‚   â”‚   â””â”€â”€ ğŸ“„ data_quality_dag.py # VerificaÃ§Ã£o qualidade
â”‚   â”œâ”€â”€ ğŸ“ plugins/                # Plugins customizados
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ custom_operators.py # Operadores customizados
â”‚   â”‚   â””â”€â”€ ğŸ“„ custom_sensors.py   # Sensores customizados
â”‚   â””â”€â”€ ğŸ“ config/                 # ConfiguraÃ§Ãµes
â”‚       â”œâ”€â”€ ğŸ“„ connections.py      # ConexÃµes
â”‚       â””â”€â”€ ğŸ“„ variables.py        # VariÃ¡veis
â”œâ”€â”€ ğŸ“ python/                     # Scripts Python
â”‚   â”œâ”€â”€ ğŸ“ etl/                    # Scripts ETL
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ extract_sources.py  # ExtraÃ§Ã£o de fontes
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ transform_data.py   # TransformaÃ§Ãµes
â”‚   â”‚   â””â”€â”€ ğŸ“„ load_warehouse.py   # Carga no DW
â”‚   â”œâ”€â”€ ğŸ“ analysis/               # AnÃ¡lises Python
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ cohort_analysis.py  # AnÃ¡lise de coorte
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ rfm_analysis.py     # AnÃ¡lise RFM
â”‚   â”‚   â””â”€â”€ ğŸ“„ time_series_analysis.py # SÃ©ries temporais
â”‚   â”œâ”€â”€ ğŸ“ utils/                  # UtilitÃ¡rios
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ database_utils.py   # UtilitÃ¡rios de banco
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ data_quality.py     # Qualidade de dados
â”‚   â”‚   â””â”€â”€ ğŸ“„ logging_utils.py    # Sistema de logs
â”‚   â””â”€â”€ ğŸ“ tests/                  # Testes Python
â”‚       â”œâ”€â”€ ğŸ“„ test_etl.py         # Testes ETL
â”‚       â””â”€â”€ ğŸ“„ test_data_quality.py # Testes qualidade
â”œâ”€â”€ ğŸ“ notebooks/                  # Jupyter notebooks
â”‚   â”œâ”€â”€ ğŸ“„ 01_data_exploration.ipynb # ExploraÃ§Ã£o de dados
â”‚   â”œâ”€â”€ ğŸ“„ 02_dimensional_modeling.ipynb # Modelagem dimensional
â”‚   â”œâ”€â”€ ğŸ“„ 03_performance_analysis.ipynb # AnÃ¡lise performance
â”‚   â””â”€â”€ ğŸ“„ 04_business_insights.ipynb # Insights de negÃ³cio
â”œâ”€â”€ ğŸ“ dashboards/                 # Dashboards e relatÃ³rios
â”‚   â”œâ”€â”€ ğŸ“ superset/              # Dashboards Superset
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ sales_dashboard.json # Dashboard vendas
â”‚   â”‚   â””â”€â”€ ğŸ“„ executive_dashboard.json # Dashboard executivo
â”‚   â”œâ”€â”€ ğŸ“ grafana/               # Dashboards Grafana
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ data_quality_dashboard.json # Qualidade dados
â”‚   â”‚   â””â”€â”€ ğŸ“„ performance_dashboard.json # Performance
â”‚   â””â”€â”€ ğŸ“ reports/               # RelatÃ³rios
â”‚       â”œâ”€â”€ ğŸ“„ monthly_report.sql  # RelatÃ³rio mensal
â”‚       â””â”€â”€ ğŸ“„ quarterly_report.sql # RelatÃ³rio trimestral
â”œâ”€â”€ ğŸ“ config/                     # ConfiguraÃ§Ãµes
â”‚   â”œâ”€â”€ ğŸ“„ database_config.yaml    # ConfiguraÃ§Ã£o banco
â”‚   â”œâ”€â”€ ğŸ“„ etl_config.yaml         # ConfiguraÃ§Ã£o ETL
â”‚   â””â”€â”€ ğŸ“„ monitoring_config.yaml  # ConfiguraÃ§Ã£o monitoramento
â”œâ”€â”€ ğŸ“ docs/                       # DocumentaÃ§Ã£o
â”‚   â”œâ”€â”€ ğŸ“„ data_dictionary.md      # DicionÃ¡rio de dados
â”‚   â”œâ”€â”€ ğŸ“„ business_rules.md       # Regras de negÃ³cio
â”‚   â””â”€â”€ ğŸ“„ architecture.md         # Arquitetura
â”œâ”€â”€ ğŸ“ docker/                     # Containers Docker
â”‚   â”œâ”€â”€ ğŸ“„ Dockerfile.postgres     # PostgreSQL
â”‚   â”œâ”€â”€ ğŸ“„ Dockerfile.airflow      # Airflow
â”‚   â””â”€â”€ ğŸ“„ docker-compose.yml      # OrquestraÃ§Ã£o
â”œâ”€â”€ ğŸ“„ requirements.txt            # DependÃªncias Python
â”œâ”€â”€ ğŸ“„ README.md                   # Este arquivo
â”œâ”€â”€ ğŸ“„ LICENSE                     # LicenÃ§a MIT
â””â”€â”€ ğŸ“„ .gitignore                 # Arquivos ignorados
```

### ğŸ“Š AnÃ¡lises DisponÃ­veis

#### 1. ğŸ“ˆ AnÃ¡lise de Vendas

**Vendas por PerÃ­odo**
```sql
-- AnÃ¡lise de vendas com window functions
WITH sales_by_month AS (
    SELECT 
        DATE_TRUNC('month', order_date) AS month,
        SUM(total_amount) AS monthly_sales,
        COUNT(DISTINCT order_id) AS order_count,
        COUNT(DISTINCT customer_id) AS unique_customers,
        AVG(total_amount) AS avg_order_value
    FROM fact_sales fs
    JOIN dim_date dd ON fs.date_key = dd.date_key
    WHERE order_date >= CURRENT_DATE - INTERVAL '24 months'
    GROUP BY DATE_TRUNC('month', order_date)
),
sales_with_growth AS (
    SELECT 
        month,
        monthly_sales,
        order_count,
        unique_customers,
        avg_order_value,
        LAG(monthly_sales) OVER (ORDER BY month) AS prev_month_sales,
        (monthly_sales - LAG(monthly_sales) OVER (ORDER BY month)) / 
        LAG(monthly_sales) OVER (ORDER BY month) * 100 AS growth_rate,
        SUM(monthly_sales) OVER (ORDER BY month ROWS UNBOUNDED PRECEDING) AS cumulative_sales
    FROM sales_by_month
)
SELECT 
    month,
    monthly_sales,
    ROUND(growth_rate, 2) AS growth_rate_pct,
    cumulative_sales,
    order_count,
    unique_customers,
    ROUND(avg_order_value, 2) AS avg_order_value
FROM sales_with_growth
ORDER BY month;
```

**AnÃ¡lise de Sazonalidade**
```sql
-- PadrÃµes sazonais de vendas
WITH seasonal_analysis AS (
    SELECT 
        EXTRACT(QUARTER FROM dd.date_actual) AS quarter,
        EXTRACT(MONTH FROM dd.date_actual) AS month,
        EXTRACT(DOW FROM dd.date_actual) AS day_of_week,
        SUM(fs.total_amount) AS total_sales,
        COUNT(fs.order_id) AS order_count,
        AVG(fs.total_amount) AS avg_order_value
    FROM fact_sales fs
    JOIN dim_date dd ON fs.date_key = dd.date_key
    WHERE dd.date_actual >= CURRENT_DATE - INTERVAL '3 years'
    GROUP BY 
        EXTRACT(QUARTER FROM dd.date_actual),
        EXTRACT(MONTH FROM dd.date_actual),
        EXTRACT(DOW FROM dd.date_actual)
)
SELECT 
    quarter,
    month,
    CASE day_of_week
        WHEN 0 THEN 'Sunday'
        WHEN 1 THEN 'Monday'
        WHEN 2 THEN 'Tuesday'
        WHEN 3 THEN 'Wednesday'
        WHEN 4 THEN 'Thursday'
        WHEN 5 THEN 'Friday'
        WHEN 6 THEN 'Saturday'
    END AS day_name,
    ROUND(AVG(total_sales), 2) AS avg_daily_sales,
    ROUND(AVG(order_count), 0) AS avg_daily_orders,
    ROUND(AVG(avg_order_value), 2) AS avg_order_value
FROM seasonal_analysis
GROUP BY quarter, month, day_of_week
ORDER BY quarter, month, day_of_week;
```

#### 2. ğŸ‘¥ AnÃ¡lise de Clientes

**SegmentaÃ§Ã£o RFM**
```sql
-- AnÃ¡lise RFM (Recency, Frequency, Monetary)
WITH customer_rfm AS (
    SELECT 
        dc.customer_id,
        dc.customer_name,
        -- Recency: dias desde a Ãºltima compra
        CURRENT_DATE - MAX(dd.date_actual) AS recency_days,
        -- Frequency: nÃºmero de pedidos
        COUNT(DISTINCT fs.order_id) AS frequency,
        -- Monetary: valor total gasto
        SUM(fs.total_amount) AS monetary_value
    FROM dim_customer dc
    JOIN fact_sales fs ON dc.customer_key = fs.customer_key
    JOIN dim_date dd ON fs.date_key = dd.date_key
    WHERE dd.date_actual >= CURRENT_DATE - INTERVAL '2 years'
    GROUP BY dc.customer_id, dc.customer_name
),
rfm_scores AS (
    SELECT 
        customer_id,
        customer_name,
        recency_days,
        frequency,
        monetary_value,
        -- Scores RFM (1-5, sendo 5 o melhor)
        NTILE(5) OVER (ORDER BY recency_days DESC) AS recency_score,
        NTILE(5) OVER (ORDER BY frequency ASC) AS frequency_score,
        NTILE(5) OVER (ORDER BY monetary_value ASC) AS monetary_score
    FROM customer_rfm
),
rfm_segments AS (
    SELECT 
        *,
        CASE 
            WHEN recency_score >= 4 AND frequency_score >= 4 AND monetary_score >= 4 
            THEN 'Champions'
            WHEN recency_score >= 3 AND frequency_score >= 3 AND monetary_score >= 3 
            THEN 'Loyal Customers'
            WHEN recency_score >= 4 AND frequency_score <= 2 
            THEN 'New Customers'
            WHEN recency_score <= 2 AND frequency_score >= 3 
            THEN 'At Risk'
            WHEN recency_score <= 2 AND frequency_score <= 2 
            THEN 'Lost Customers'
            ELSE 'Potential Loyalists'
        END AS customer_segment
    FROM rfm_scores
)
SELECT 
    customer_segment,
    COUNT(*) AS customer_count,
    ROUND(AVG(recency_days), 1) AS avg_recency,
    ROUND(AVG(frequency), 1) AS avg_frequency,
    ROUND(AVG(monetary_value), 2) AS avg_monetary,
    ROUND(SUM(monetary_value), 2) AS total_revenue,
    ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER (), 2) AS segment_percentage
FROM rfm_segments
GROUP BY customer_segment
ORDER BY total_revenue DESC;
```

**AnÃ¡lise de Coorte**
```sql
-- AnÃ¡lise de coorte de retenÃ§Ã£o
WITH customer_cohorts AS (
    SELECT 
        dc.customer_id,
        MIN(dd.date_actual) AS first_purchase_date,
        DATE_TRUNC('month', MIN(dd.date_actual)) AS cohort_month
    FROM dim_customer dc
    JOIN fact_sales fs ON dc.customer_key = fs.customer_key
    JOIN dim_date dd ON fs.date_key = dd.date_key
    GROUP BY dc.customer_id
),
customer_activities AS (
    SELECT 
        cc.customer_id,
        cc.cohort_month,
        DATE_TRUNC('month', dd.date_actual) AS activity_month,
        EXTRACT(EPOCH FROM (DATE_TRUNC('month', dd.date_actual) - cc.cohort_month)) / 
        EXTRACT(EPOCH FROM INTERVAL '1 month') AS period_number
    FROM customer_cohorts cc
    JOIN fact_sales fs ON cc.customer_id = fs.customer_id
    JOIN dim_date dd ON fs.date_key = dd.date_key
),
cohort_table AS (
    SELECT 
        cohort_month,
        period_number,
        COUNT(DISTINCT customer_id) AS customers
    FROM customer_activities
    GROUP BY cohort_month, period_number
),
cohort_sizes AS (
    SELECT 
        cohort_month,
        COUNT(DISTINCT customer_id) AS cohort_size
    FROM customer_cohorts
    GROUP BY cohort_month
)
SELECT 
    ct.cohort_month,
    cs.cohort_size,
    ct.period_number,
    ct.customers,
    ROUND(100.0 * ct.customers / cs.cohort_size, 2) AS retention_rate
FROM cohort_table ct
JOIN cohort_sizes cs ON ct.cohort_month = cs.cohort_month
WHERE ct.period_number <= 12  -- Primeiros 12 meses
ORDER BY ct.cohort_month, ct.period_number;
```

#### 3. ğŸ“¦ AnÃ¡lise de Produtos

**Performance de Produtos**
```sql
-- AnÃ¡lise de performance de produtos com ranking
WITH product_performance AS (
    SELECT 
        dp.product_id,
        dp.product_name,
        dp.category,
        dp.subcategory,
        SUM(fs.quantity) AS total_quantity_sold,
        SUM(fs.total_amount) AS total_revenue,
        COUNT(DISTINCT fs.order_id) AS order_count,
        COUNT(DISTINCT fs.customer_id) AS unique_customers,
        AVG(fs.unit_price) AS avg_unit_price,
        SUM(fs.total_amount) / SUM(fs.quantity) AS avg_selling_price
    FROM dim_product dp
    JOIN fact_sales fs ON dp.product_key = fs.product_key
    JOIN dim_date dd ON fs.date_key = dd.date_key
    WHERE dd.date_actual >= CURRENT_DATE - INTERVAL '12 months'
    GROUP BY dp.product_id, dp.product_name, dp.category, dp.subcategory
),
product_rankings AS (
    SELECT 
        *,
        ROW_NUMBER() OVER (ORDER BY total_revenue DESC) AS revenue_rank,
        ROW_NUMBER() OVER (ORDER BY total_quantity_sold DESC) AS quantity_rank,
        ROW_NUMBER() OVER (ORDER BY unique_customers DESC) AS customer_reach_rank,
        PERCENT_RANK() OVER (ORDER BY total_revenue) AS revenue_percentile
    FROM product_performance
)
SELECT 
    product_name,
    category,
    subcategory,
    total_quantity_sold,
    ROUND(total_revenue, 2) AS total_revenue,
    order_count,
    unique_customers,
    ROUND(avg_selling_price, 2) AS avg_selling_price,
    revenue_rank,
    quantity_rank,
    customer_reach_rank,
    ROUND(revenue_percentile * 100, 2) AS revenue_percentile
FROM product_rankings
WHERE revenue_rank <= 50  -- Top 50 produtos
ORDER BY revenue_rank;
```

#### 4. ğŸ’° AnÃ¡lise Financeira

**KPIs Financeiros**
```sql
-- Dashboard de KPIs financeiros
WITH financial_metrics AS (
    SELECT 
        DATE_TRUNC('month', dd.date_actual) AS month,
        SUM(fs.total_amount) AS gross_revenue,
        SUM(fs.quantity * dp.cost_price) AS total_cost,
        SUM(fs.total_amount) - SUM(fs.quantity * dp.cost_price) AS gross_profit,
        COUNT(DISTINCT fs.order_id) AS total_orders,
        COUNT(DISTINCT fs.customer_id) AS unique_customers,
        SUM(fs.quantity) AS total_units_sold
    FROM fact_sales fs
    JOIN dim_date dd ON fs.date_key = dd.date_key
    JOIN dim_product dp ON fs.product_key = dp.product_key
    WHERE dd.date_actual >= CURRENT_DATE - INTERVAL '24 months'
    GROUP BY DATE_TRUNC('month', dd.date_actual)
),
metrics_with_calculations AS (
    SELECT 
        month,
        gross_revenue,
        total_cost,
        gross_profit,
        ROUND(100.0 * gross_profit / gross_revenue, 2) AS gross_margin_pct,
        total_orders,
        unique_customers,
        total_units_sold,
        ROUND(gross_revenue / total_orders, 2) AS avg_order_value,
        ROUND(gross_revenue / unique_customers, 2) AS avg_customer_value,
        ROUND(total_orders::DECIMAL / unique_customers, 2) AS avg_orders_per_customer
    FROM financial_metrics
)
SELECT 
    month,
    gross_revenue,
    total_cost,
    gross_profit,
    gross_margin_pct,
    total_orders,
    unique_customers,
    total_units_sold,
    avg_order_value,
    avg_customer_value,
    avg_orders_per_customer,
    -- ComparaÃ§Ã£o com mÃªs anterior
    LAG(gross_revenue) OVER (ORDER BY month) AS prev_month_revenue,
    ROUND(100.0 * (gross_revenue - LAG(gross_revenue) OVER (ORDER BY month)) / 
          LAG(gross_revenue) OVER (ORDER BY month), 2) AS revenue_growth_pct
FROM metrics_with_calculations
ORDER BY month DESC;
```

### ğŸ”„ ETL com dbt

#### Modelo Staging
```sql
-- models/staging/stg_orders.sql
{{ config(materialized='view') }}

WITH source_orders AS (
    SELECT * FROM {{ source('ecommerce', 'orders') }}
),

cleaned_orders AS (
    SELECT
        order_id,
        customer_id,
        order_date::DATE AS order_date,
        CASE 
            WHEN status IN ('completed', 'shipped', 'delivered') THEN 'completed'
            WHEN status IN ('cancelled', 'refunded') THEN 'cancelled'
            ELSE 'pending'
        END AS order_status,
        total_amount::DECIMAL(10,2) AS total_amount,
        shipping_cost::DECIMAL(10,2) AS shipping_cost,
        tax_amount::DECIMAL(10,2) AS tax_amount,
        discount_amount::DECIMAL(10,2) AS discount_amount,
        created_at,
        updated_at
    FROM source_orders
    WHERE order_date IS NOT NULL
      AND total_amount > 0
)

SELECT * FROM cleaned_orders
```

#### Modelo Dimensional
```sql
-- models/marts/dim_customer.sql
{{ config(
    materialized='table',
    indexes=[
      {'columns': ['customer_id'], 'unique': True},
      {'columns': ['email'], 'unique': True}
    ]
) }}

WITH customer_base AS (
    SELECT * FROM {{ ref('stg_customers') }}
),

customer_metrics AS (
    SELECT 
        customer_id,
        MIN(order_date) AS first_order_date,
        MAX(order_date) AS last_order_date,
        COUNT(DISTINCT order_id) AS total_orders,
        SUM(total_amount) AS lifetime_value,
        AVG(total_amount) AS avg_order_value
    FROM {{ ref('stg_orders') }}
    WHERE order_status = 'completed'
    GROUP BY customer_id
),

final AS (
    SELECT 
        {{ dbt_utils.generate_surrogate_key(['cb.customer_id']) }} AS customer_key,
        cb.customer_id,
        cb.first_name,
        cb.last_name,
        cb.email,
        cb.phone,
        cb.address,
        cb.city,
        cb.state,
        cb.country,
        cb.postal_code,
        cb.registration_date,
        COALESCE(cm.first_order_date, cb.registration_date) AS first_order_date,
        cm.last_order_date,
        COALESCE(cm.total_orders, 0) AS total_orders,
        COALESCE(cm.lifetime_value, 0) AS lifetime_value,
        COALESCE(cm.avg_order_value, 0) AS avg_order_value,
        CASE 
            WHEN cm.total_orders >= 10 THEN 'VIP'
            WHEN cm.total_orders >= 5 THEN 'Regular'
            WHEN cm.total_orders >= 1 THEN 'New'
            ELSE 'Prospect'
        END AS customer_tier,
        CURRENT_TIMESTAMP AS dbt_updated_at
    FROM customer_base cb
    LEFT JOIN customer_metrics cm ON cb.customer_id = cm.customer_id
)

SELECT * FROM final
```

### ğŸ¯ CompetÃªncias Demonstradas

#### SQL AvanÃ§ado
- âœ… **Window Functions**: ROW_NUMBER, RANK, LAG/LEAD, NTILE
- âœ… **CTEs**: Common Table Expressions complexas
- âœ… **AgregaÃ§Ãµes**: GROUP BY, HAVING, ROLLUP, CUBE
- âœ… **Joins**: INNER, LEFT, RIGHT, FULL OUTER, CROSS

#### Data Warehousing
- âœ… **Modelagem Dimensional**: Star schema, snowflake
- âœ… **ETL/ELT**: ExtraÃ§Ã£o, transformaÃ§Ã£o, carga
- âœ… **SCD**: Slowly Changing Dimensions
- âœ… **Particionamento**: OtimizaÃ§Ã£o de performance

#### Ferramentas Modernas
- âœ… **dbt**: TransformaÃ§Ãµes como cÃ³digo
- âœ… **Airflow**: OrquestraÃ§Ã£o de workflows
- âœ… **Great Expectations**: Qualidade de dados
- âœ… **Apache Superset**: VisualizaÃ§Ã£o e BI

### ğŸš€ Setup RÃ¡pido

#### Docker Compose
```bash
# Clonar repositÃ³rio
git clone https://github.com/galafis/sql-data-warehouse-analysis.git
cd sql-data-warehouse-analysis

# Iniciar ambiente completo
docker-compose up -d

# ServiÃ§os disponÃ­veis:
# - PostgreSQL: localhost:5432
# - Airflow: http://localhost:8080
# - Superset: http://localhost:8088
# - Jupyter: http://localhost:8888
```

#### ConfiguraÃ§Ã£o dbt
```bash
# Instalar dbt
pip install dbt-postgres

# Configurar perfil
dbt init warehouse_analytics

# Executar transformaÃ§Ãµes
dbt run

# Gerar documentaÃ§Ã£o
dbt docs generate
dbt docs serve
```

### ğŸ“ˆ Casos de Uso PrÃ¡ticos

#### 1. E-commerce Analytics
- AnÃ¡lise de funil de vendas
- SegmentaÃ§Ã£o de clientes
- AnÃ¡lise de coorte de retenÃ§Ã£o
- Performance de produtos

#### 2. Financial Services
- AnÃ¡lise de risco de crÃ©dito
- DetecÃ§Ã£o de fraude
- AnÃ¡lise de rentabilidade
- Compliance regulatÃ³rio

#### 3. Healthcare Analytics
- AnÃ¡lise de desfechos clÃ­nicos
- OtimizaÃ§Ã£o de recursos
- AnÃ¡lise epidemiolÃ³gica
- Qualidade de atendimento

#### 4. Supply Chain
- AnÃ¡lise de demanda
- OtimizaÃ§Ã£o de estoque
- Performance de fornecedores
- AnÃ¡lise de custos logÃ­sticos

---

## ğŸ‡ºğŸ‡¸ English

### ğŸ“Š Overview

Comprehensive **data warehouse analysis platform** developed with advanced SQL and modern ETL tools:

- ğŸ—ï¸ **Dimensional Architecture**: Star schema, snowflake, data vault
- ğŸ“Š **Advanced SQL**: CTEs, window functions, complex analyses
- ğŸ”„ **Modern ETL**: dbt for transformations, Airflow for orchestration
- ğŸ“ˆ **Analytics**: KPIs, business metrics, temporal analyses
- ğŸ¯ **Performance**: Query optimization, indexing, partitioning
- ğŸ“‹ **Governance**: Data quality, lineage, documentation

### ğŸ¯ Platform Objectives

- **Centralize data** from multiple operational sources
- **Standardize metrics** and business definitions
- **Accelerate analyses** with optimized structures
- **Ensure quality** and data consistency
- **Facilitate self-service** analytics for users

### ğŸ“Š Available Analyses

#### 1. ğŸ“ˆ Sales Analysis
- Sales by period with growth rates
- Seasonality patterns
- Product performance ranking
- Customer segmentation

#### 2. ğŸ‘¥ Customer Analysis
- RFM segmentation (Recency, Frequency, Monetary)
- Cohort retention analysis
- Customer lifetime value
- Churn prediction

#### 3. ğŸ“¦ Product Analysis
- Product performance metrics
- Category analysis
- Inventory optimization
- Price elasticity

#### 4. ğŸ’° Financial Analysis
- Financial KPIs dashboard
- Profitability analysis
- Cost analysis
- Revenue forecasting

### ğŸ¯ Skills Demonstrated

#### Advanced SQL
- âœ… **Window Functions**: ROW_NUMBER, RANK, LAG/LEAD, NTILE
- âœ… **CTEs**: Complex Common Table Expressions
- âœ… **Aggregations**: GROUP BY, HAVING, ROLLUP, CUBE
- âœ… **Joins**: INNER, LEFT, RIGHT, FULL OUTER, CROSS

#### Data Warehousing
- âœ… **Dimensional Modeling**: Star schema, snowflake
- âœ… **ETL/ELT**: Extract, transform, load
- âœ… **SCD**: Slowly Changing Dimensions
- âœ… **Partitioning**: Performance optimization

#### Modern Tools
- âœ… **dbt**: Transformations as code
- âœ… **Airflow**: Workflow orchestration
- âœ… **Great Expectations**: Data quality
- âœ… **Apache Superset**: Visualization and BI

---

## ğŸ“„ LicenÃ§a | License

MIT License - veja o arquivo [LICENSE](LICENSE) para detalhes | see [LICENSE](LICENSE) file for details

## ğŸ“ Contato | Contact

**GitHub**: [@galafis](https://github.com/galafis)  
**LinkedIn**: [Gabriel Demetrios Lafis](https://linkedin.com/in/galafis)  
**Email**: gabriel.lafis@example.com

---

<div align="center">

**Desenvolvido com â¤ï¸ para Analytics Empresarial | Developed with â¤ï¸ for Enterprise Analytics**

[![GitHub](https://img.shields.io/badge/GitHub-galafis-blue?style=flat-square&logo=github)](https://github.com/galafis)
[![SQL](https://img.shields.io/badge/SQL-4479A1?style=flat-square&logo=postgresql&logoColor=white)](https://www.postgresql.org/)

</div>

